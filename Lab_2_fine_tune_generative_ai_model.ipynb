{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b80e74a-dd44-42ec-a63f-cb656a533801",
   "metadata": {},
   "source": [
    "## Fine-Tune a Generative AI Model for Dialogue Summarization\n",
    "\n",
    "In this notebook, you will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the FLAN-T5 model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics.\n",
    "\n",
    "### Table of Contents\n",
    "* Load Dataset and LLM\n",
    "  * Load Dataset and LLM\n",
    "  * Test the Model with Zero Shot Inferencing\n",
    "* Perform Full Fine-Tuning\n",
    "  * Preprocess the Dialog-Summary Dataset\n",
    "  * Fine-Tune the Model with the Preprocessed Dataset\n",
    "  * Evaluate the Model Qualitatively (Human Evaluation)\n",
    "  * Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "* Perform Parameter Efficient Fine-Tuning (PEFT)\n",
    "  * Setup the PEFT/LoRA model for Fine-Tuning\n",
    "  * Train PEFT Adapter\n",
    "  * Evaluate the Model Qualitatively (Human Evaluation)\n",
    "  * Evaluate the Model Quantitatively (with ROUGE Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cda4955-6bb6-4618-9fb6-c7dc35bda7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/usr/local/cuda/compat/lib': Read-only file system\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: loralib in /opt/conda/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.14.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.17.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (2.0.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.13.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.34.1)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (13.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft) (68.2.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft) (0.41.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (2023.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.14.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "rm: cannot remove '/usr/local/cuda/compat/lib': Read-only file system\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: scikit-learn in /home01/qualis/.local/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.3)\n",
      "Requirement already satisfied: bitsandbytes in /home01/qualis/.local/lib/python3.10/site-packages (0.42.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home01/qualis/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#%pip install --upgrade pip\n",
    "#%pip install --disable-pip-version-check \\\n",
    "#    torch==1.13.1 \\\n",
    "#    torchdata==0.5.1 --quiet\n",
    "#\n",
    "#%pip install \\\n",
    "#    transformers==4.27.2 \\\n",
    "#    datasets==2.11.0 \\\n",
    "#    evaluate==0.4.0 \\\n",
    "#    rouge_score==0.1.2 \\\n",
    "#    loralib==0.1.1 \\\n",
    "#    peft==0.3.0 --quiet\n",
    "!pip install evaluate rouge_score loralib peft\n",
    "!pip install scikit-learn scipy bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "078aed76-f228-44c7-921e-d9ac5b4ee664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f899886-3889-4a5e-af70-d9e7a0c14cf8",
   "metadata": {},
   "source": [
    "#### Load Dataset and LLM\n",
    "You are going to continue experimenting with the DialogSum Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2888af5-c2b5-46c2-8103-5a79e94de9ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3246cf5-de70-41cc-997b-5e0f2dbe7e1b",
   "metadata": {},
   "source": [
    "##### Load the pre-trained FLAN-T5 model and its tokenizer directly from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-base) of FLAN-T5. Setting torch_dtype=torch.bfloat16 specifies the memory type to be used by this model. \n",
    "\n",
    "##### In the AutoModelForCausalLM class of the  Transformers library, the torch_dtype argument specifies the data type used for internal computations and storage in the model. It can take several values:\n",
    "\n",
    "- None (default): This uses the default dtype for PyTorch Tensors, which is currently torch.float32.\n",
    "- A valid torch dtype: You can explicitly specify a different dtype like torch.float16 or torch.bfloat16 for lower memory usage and faster computation.\n",
    "- \"auto\": This option automatically tries to infer the appropriate dtype based on the loaded model weights. If the weights are already in a specific dtype, it will use that. Otherwise, it falls back to the default (torch.float32).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d30d4b99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name='google/flan-t5-base'\n",
    "#model_name='google/flan-t5-large'\n",
    "#model_name='google/flan-t5-xl'\n",
    "#model_name='google/flan-t5-xxl'\n",
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd137d6e-2fe9-476d-9018-608079d02eca",
   "metadata": {},
   "source": [
    "##### It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7858c076-59da-4d94-b791-76bd7da8ac6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb933c-81f5-4131-9594-0e5f1d7be1a8",
   "metadata": {},
   "source": [
    "#### Test the Model with Zero Shot Inferencing\n",
    "Test the model with the zero shot inferencing. You can see that the model struggles to summarize the dialogue compared to the baseline summary, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc15e830-718b-411b-843e-353db61820ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "original_model = original_model.to(device)\n",
    "index = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b2b4a-af62-473a-ac02-0e6f46e2e6b5",
   "metadata": {},
   "source": [
    "### Perform Full Fine-Tuning\n",
    "\n",
    "#### Preprocess the Dialog-Summary Dataset\n",
    "You need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Preprocess the prompt-response dataset into tokens and pull out their input_ids (1 per token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "664aadbd-1dbf-4c85-8002-c167b3274044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59de01e4-68cc-483c-b05a-cda486d05e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b343b02-7cc0-4021-9767-176aed57f81e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb2237ae-ae38-4546-abdd-dded4151e8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce9f1cbf-2b62-4254-b37b-141d1d074c64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37cbfbdf-3463-4d10-b128-f25048c1f86e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (12460, 2)\n",
      "Validation: (500, 2)\n",
      "Test: (1500, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea966292-6a71-41cd-b9ad-20319a6de813",
   "metadata": {},
   "source": [
    "#### Fine-Tune the Model with the Preprocessed Dataset\n",
    "Now utilize the built-in Hugging Face Trainer class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fc718e6-6cac-4477-9c2d-4972594139c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "#output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "output_dir = \"./dialogue-summary-training\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    #learning_rate=1e-5,\n",
    "    learning_rate=2e-5,\n",
    "    #learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    #num_train_epochs=45,\n",
    "    #num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    fp16 = True,\n",
    "    #max_steps=1,\n",
    "    max_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6337554-9149-4e63-b346-dded3e2a8a91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.0, metrics={'train_runtime': 19.1154, 'train_samples_per_second': 41.851, 'train_steps_per_second': 5.231, 'total_flos': 547805881958400.0, 'train_loss': 0.0, 'epoch': 0.06})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() #It took ~14 minutes with fp32 and 1 epoch\n",
    "\n",
    "#trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "188889f2-78d5-43c1-978d-7955d27f6ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fa82935-104c-42ad-bbdb-7add2fbc4344",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
       " './dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
       " './dialogue-summary-checkpoint-local/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_model_path=\"./dialogue-summary-checkpoint-local\"\n",
    "\n",
    "trainer.model.save_pretrained(instruct_model_path)\n",
    "tokenizer.save_pretrained(instruct_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bc003c6-e3a2-44c5-97ef-eb6e474fbd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -al ./dialogue-summary-checkpoint-local\n",
    "#total 474179\n",
    "#drwxr-xr-x  2 qualis in0183      8192 Dec 17 15:14 .\n",
    "#drwxr-xr-x 15 qualis in0183      8192 Dec 17 15:14 ..\n",
    "#-rw-r--r--  1 qualis in0183      1558 Dec 17 15:14 config.json\n",
    "#-rw-r--r--  1 qualis in0183       142 Dec 17 15:14 generation_config.json\n",
    "#-rw-r--r--  1 qualis in0183 495189552 Dec 17 15:14 model.safetensors\n",
    "#-rw-r--r--  1 qualis in0183      2543 Dec 17 15:14 special_tokens_map.json\n",
    "#-rw-r--r--  1 qualis in0183     20771 Dec 17 15:14 tokenizer_config.json\n",
    "#-rw-r--r--  1 qualis in0183   2422256 Dec 17 15:14 tokenizer.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f39d306-aed7-4bc8-b7ca-28b3297986de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(instruct_model_path, torch_dtype=torch.bfloat16)\n",
    "#instruct_model = AutoModelForSeq2SeqLM.from_pretrained(instruct_model_path)\n",
    "#instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-dialogue-summary-checkpoint\", torch_dtype=torch.bfloat16)\n",
    "#instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./fine-tune-test/dialogue-summary-checkpoint-local\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52872c2e-673d-4e23-8374-78635eab3286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct_model = instruct_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d1d4ce4-8bb5-4133-a833-46c6a042934b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(instruct_model)) #trainable model parameters: 247577856"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f1bb2-2754-4871-b99a-0a47a27274a8",
   "metadata": {},
   "source": [
    "#### Evaluate the Model Qualitatively (Human Evaluation)\n",
    "As with many GenAI applications, a qualitative approach where you ask yourself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), you can see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10e0b348-ccc7-4572-8531-34e3e40c3e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "The train leaves for the railway station at ten to thirty minutes late.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "The train is about to leave.\n"
     ]
    }
   ],
   "source": [
    "#index = 200\n",
    "index = 40\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a63a46-02bb-4f5c-a457-98080c954689",
   "metadata": {},
   "source": [
    "#### Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "The ROUGE metric helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b47c89d8-2c26-4323-98f9-eb30d8ea39f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e433a01-a86f-4d44-85a0-dc280b4a89a9",
   "metadata": {},
   "source": [
    "##### Generate the outputs for the sample of the test dataset (only 10 dialogues and summaries to save time), and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9aa2fb38-3c1d-4fe7-b317-decf033c3f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>The following are the most recent memos from t...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>Employees will be required to use instant mess...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>#Person1: Your new memo is in the mail.</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>The driver is a little worried about the traff...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The conversation lasted about an hour.</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>Brian's birthday is coming up, so he's invited...</td>\n",
       "      <td>#Person1#: Happy birthday, Brian. #Person2#: I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#Person1# attends Brian's birthday party. Bria...</td>\n",
       "      <td>Brian has a birthday party.</td>\n",
       "      <td>#Person1#: Happy birthday, Brian. #Person2#: I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#Person1# has a dance with Brian at Brian's bi...</td>\n",
       "      <td>Brian is celebrating his birthday.</td>\n",
       "      <td>#Person1#: Happy birthday, Brian. #Person2#: I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#Person1# is surprised at the Olympic Stadium'...</td>\n",
       "      <td>The whole Olympic park is to be finished this ...</td>\n",
       "      <td>The Olympic park is a huge stadium.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>#Person2# shows #Person1# around the construct...</td>\n",
       "      <td>The Olympic Stadium is a big stadium.</td>\n",
       "      <td>The Olympic park is a huge stadium.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>#Person2# introduces the Olympic Stadium's fin...</td>\n",
       "      <td>The whole stadium is to be finished in June.</td>\n",
       "      <td>The Olympic park is a huge stadium.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>#Person1# wants to create a company and is goi...</td>\n",
       "      <td>#Person1: I am going to start a business. #Per...</td>\n",
       "      <td>#Person1#: I am done working for a company tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>#Person1# abandons the idea of creating a comp...</td>\n",
       "      <td>#Person1#: I am done working for a company tha...</td>\n",
       "      <td>#Person1#: I am done working for a company tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>#Person1# wants to start #Person1#'s own busin...</td>\n",
       "      <td>#Person1: I'm done working for a company that ...</td>\n",
       "      <td>#Person1#: I am done working for a company tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>#Person2# feels itchy. #Person1# doubts it is ...</td>\n",
       "      <td>Person1 is feeling itchy and lightheaded.</td>\n",
       "      <td>#Person1#: I'm scratching so much. I can't sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#Person1# suspects that #Person2# has chicken ...</td>\n",
       "      <td>: I feel lightheaded and weak.</td>\n",
       "      <td>#Person1#: I'm scratching so much. I can't sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             human_baseline_summaries  \\\n",
       "0   Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1   In order to prevent employees from wasting tim...   \n",
       "2   Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3   #Person2# arrives late because of traffic jam....   \n",
       "4   #Person2# decides to follow #Person1#'s sugges...   \n",
       "5   #Person2# complains to #Person1# about the tra...   \n",
       "6   #Person1# tells Kate that Masha and Hero get d...   \n",
       "7   #Person1# tells Kate that Masha and Hero are g...   \n",
       "8   #Person1# and Kate talk about the divorce betw...   \n",
       "9   #Person1# and Brian are at the birthday party ...   \n",
       "10  #Person1# attends Brian's birthday party. Bria...   \n",
       "11  #Person1# has a dance with Brian at Brian's bi...   \n",
       "12  #Person1# is surprised at the Olympic Stadium'...   \n",
       "13  #Person2# shows #Person1# around the construct...   \n",
       "14  #Person2# introduces the Olympic Stadium's fin...   \n",
       "15  #Person1# wants to create a company and is goi...   \n",
       "16  #Person1# abandons the idea of creating a comp...   \n",
       "17  #Person1# wants to start #Person1#'s own busin...   \n",
       "18  #Person2# feels itchy. #Person1# doubts it is ...   \n",
       "19  #Person1# suspects that #Person2# has chicken ...   \n",
       "\n",
       "                             original_model_summaries  \\\n",
       "0   The following are the most recent memos from t...   \n",
       "1   Employees will be required to use instant mess...   \n",
       "2             #Person1: Your new memo is in the mail.   \n",
       "3   The driver is a little worried about the traff...   \n",
       "4              The conversation lasted about an hour.   \n",
       "5   The traffic jam at the Carrefour intersection ...   \n",
       "6                Masha and Hero are getting divorced.   \n",
       "7                Masha and Hero are getting divorced.   \n",
       "8                Masha and Hero are getting divorced.   \n",
       "9   Brian's birthday is coming up, so he's invited...   \n",
       "10                        Brian has a birthday party.   \n",
       "11                 Brian is celebrating his birthday.   \n",
       "12  The whole Olympic park is to be finished this ...   \n",
       "13              The Olympic Stadium is a big stadium.   \n",
       "14       The whole stadium is to be finished in June.   \n",
       "15  #Person1: I am going to start a business. #Per...   \n",
       "16  #Person1#: I am done working for a company tha...   \n",
       "17  #Person1: I'm done working for a company that ...   \n",
       "18          Person1 is feeling itchy and lightheaded.   \n",
       "19                     : I feel lightheaded and weak.   \n",
       "\n",
       "                             instruct_model_summaries  \n",
       "0      #Person1#: I need to take a dictation for you.  \n",
       "1      #Person1#: I need to take a dictation for you.  \n",
       "2      #Person1#: I need to take a dictation for you.  \n",
       "3   The traffic jam at the Carrefour intersection ...  \n",
       "4   The traffic jam at the Carrefour intersection ...  \n",
       "5   The traffic jam at the Carrefour intersection ...  \n",
       "6                Masha and Hero are getting divorced.  \n",
       "7                Masha and Hero are getting divorced.  \n",
       "8                Masha and Hero are getting divorced.  \n",
       "9   #Person1#: Happy birthday, Brian. #Person2#: I...  \n",
       "10  #Person1#: Happy birthday, Brian. #Person2#: I...  \n",
       "11  #Person1#: Happy birthday, Brian. #Person2#: I...  \n",
       "12                The Olympic park is a huge stadium.  \n",
       "13                The Olympic park is a huge stadium.  \n",
       "14                The Olympic park is a huge stadium.  \n",
       "15  #Person1#: I am done working for a company tha...  \n",
       "16  #Person1#: I am done working for a company tha...  \n",
       "17  #Person1#: I am done working for a company tha...  \n",
       "18  #Person1#: I'm scratching so much. I can't sta...  \n",
       "19  #Person1#: I'm scratching so much. I can't sta...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dialogues = dataset['test'][0:10]['dialogue']\n",
    "#human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "dialogues = dataset['test'][0:20]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:20]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    #input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    \n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a197e6d-0ed3-4b3a-b6d0-816d22f69549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(type(human_baseline_summaries))\n",
    "#print(len(human_baseline_summaries))\n",
    "#print(human_baseline_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f5c74e5-5f34-495b-82d6-c5bab89f1eef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2676874932903251, 'rouge2': 0.07378500604339167, 'rougeL': 0.22092720901555632, 'rougeLsum': 0.21935374735544388}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.24220390533454023, 'rouge2': 0.08224839727914257, 'rougeL': 0.21504808016422822, 'rougeLsum': 0.21296507931938177}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66bb366-4d46-49bc-8afa-2982fca90e1c",
   "metadata": {},
   "source": [
    "The file **Generative-AI-with-LLMs/data/dialogue-summary-training-results.csv** contains a pre-populated list of all model results which you can use to evaluate on a larger section of data. Let's do that for each of the models. The length of results is 1500. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84885586-f3b6-47e7-8bea-d2bb2979c0f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\n"
     ]
    }
   ],
   "source": [
    "#results = pd.read_csv(\"data/dialogue-summary-training-results.csv\")\n",
    "#results = pd.read_csv(\"git-projects/Generative-AI-with-LLMs/Week-2/data/dialogue-summary-training-results.csv\")\n",
    "results = pd.read_csv(\"/scratch/qualis/git-projects/Generative-AI-with-LLMs/data/dialogue-summary-training-results.csv\")\n",
    "\n",
    "human_baseline_summaries = results['human_baseline_summaries'].values\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "instruct_model_summaries = results['instruct_model_summaries'].values\n",
    "\n",
    "print(len(instruct_model_summaries))\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "398e0506-7423-489f-bef1-8ec1d27d03c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(results) # 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96167ad3-518f-4e85-9f40-8f7857041265",
   "metadata": {},
   "source": [
    "##### The results show substantial improvement in all ROUGE metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2db11276-1e29-4e3f-8468-ddfb83de2be0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\n",
      "rouge1: 18.82%\n",
      "rouge2: 10.43%\n",
      "rougeL: 13.70%\n",
      "rougeLsum: 13.69%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\")\n",
    "\n",
    "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(instruct_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a39ef-744a-49c4-96b4-f89b48656ba7",
   "metadata": {},
   "source": [
    "### Perform Parameter Efficient Fine-Tuning (PEFT)\n",
    "Now, let's perform Parameter Efficient Fine-Tuning (PEFT) fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon.\n",
    "\n",
    "PEFT is a generic term that includes Low-Rank Adaptation (LoRA) and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained “LoRA adapter” emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).\n",
    "\n",
    "That said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request. The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases.\n",
    "\n",
    "\n",
    "#### Setup the PEFT/LoRA model for Fine-Tuning\n",
    "You need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (r) hyper-parameter, which defines the rank/dimension of the adapter to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "843ff8fa-5893-4ca2-aa15-7a77be34c897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6df6001-8591-458a-8076-846c118435ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91953e8d-858c-4de4-9c35-4448f67b299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 768)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28351d7f-98c8-4d38-a957-53f230db70b9",
   "metadata": {},
   "source": [
    "#### Train PEFT Adapter\n",
    "Define training arguments and create Trainer instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37465648-96db-4415-8d15-de00e87c4cab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./peft-dialogue-summary-training\"\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=2,\n",
    "    fp16 = True,\n",
    "    logging_steps=500,\n",
    "    max_steps=100    \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9ef3ecd-0242-4d0b-994e-8daf2f9d5263",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.0, metrics={'train_runtime': 18.7164, 'train_samples_per_second': 42.743, 'train_steps_per_second': 5.343, 'total_flos': 556503190732800.0, 'train_loss': 0.0, 'epoch': 0.06})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train() #It took ~21 minutes with fp32 and 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6e50c47-69e5-4808-8dc8-519cb13bae74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
       " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
       " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627c140-eaf1-4e32-baac-a52a08f04755",
   "metadata": {},
   "source": [
    "##### Prepare this model by adding an adapter to the original FLAN-T5 model. You are setting is_trainable=False because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set is_trainable=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f82fcb13-26c1-4516-b399-f291e12e3690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "#peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\", torch_dtype=torch.bfloat16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       './peft-dialogue-summary-checkpoint-local/',  \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa84ad4-f5a0-4093-b7c5-05ed9f55981a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### The number of trainable parameters will be 0 due to is_trainable=False setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48b266e2-e661-4e89-bdbb-1c9cda3bea52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b28936-c5b7-41d6-bdcd-35623139608e",
   "metadata": {},
   "source": [
    "#### Evaluate the Model Qualitatively (Human Evaluation)\n",
    "Make inferences for the same example as in sections 1.3 and 2.3, with the original model, fully fine-tuned and PEFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8964d5e-7fbb-476e-b922-29c3084abb56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "Tom is off at 9.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "The train is about to leave.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: The train is about to leave.\n"
     ]
    }
   ],
   "source": [
    "peft_model = peft_model.to(device)\n",
    "\n",
    "#index = 200\n",
    "index = 40\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "baseline_human_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5422c2-27b2-463e-982a-c23bdf7d5033",
   "metadata": {},
   "source": [
    "#### Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "Perform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27113836-fbfe-41db-a7de-6291250cc084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>Person1 is having a rash or an allergy.</td>\n",
       "      <td>#Person1#: I'm scratching so much. I can't sta...</td>\n",
       "      <td>#Person1#: I'm scratching so much. I can't sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>The bill for the laundry service on Nov. 20th ...</td>\n",
       "      <td>#Person1#: Good morning. I'm checking out toda...</td>\n",
       "      <td>#Person1#: Good morning. I'm checking out toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>The bill has been charged for your laundry ser...</td>\n",
       "      <td>#Person1#: Good morning. I'm checking out toda...</td>\n",
       "      <td>#Person1#: Good morning. I'm checking out toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>#Person1#: Good morning. I'm in Room 309. #Per...</td>\n",
       "      <td>#Person1#: Good morning. I'm checking out toda...</td>\n",
       "      <td>#Person1#: Good morning. I'm checking out toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>Steven is upset at his wife for cheating on hi...</td>\n",
       "      <td>#Person1#: I need your help.</td>\n",
       "      <td>#Person1#: I need your help.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>#Person1: I need your help. #Person2: I need y...</td>\n",
       "      <td>#Person1#: I need your help.</td>\n",
       "      <td>#Person1#: I need your help.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>#Person1#: My wife has an affair with her secr...</td>\n",
       "      <td>#Person1#: I need your help.</td>\n",
       "      <td>#Person1#: I need your help.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Then, think of famous people.</td>\n",
       "      <td>I think Abraham Lincoln is a great man or woma...</td>\n",
       "      <td>I think Abraham Lincoln is a great man or woma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>#Person1: Abraham Lincoln. #Person2: I think o...</td>\n",
       "      <td>I think Abraham Lincoln is a great man or woma...</td>\n",
       "      <td>I think Abraham Lincoln is a great man or woma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>#Person1#: I admire the way he has a great vis...</td>\n",
       "      <td>I think Abraham Lincoln is a great man or woma...</td>\n",
       "      <td>I think Abraham Lincoln is a great man or woma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#Person1# attends Brian's birthday party. Bria...</td>\n",
       "      <td>#Person1: I'm going to Hebei for my trip. #Per...</td>\n",
       "      <td>The sandstorms in China are causing a lot of h...</td>\n",
       "      <td>The sandstorms in China are causing a lot of h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#Person1# has a dance with Brian at Brian's bi...</td>\n",
       "      <td>The North of China is experiencing severe sand...</td>\n",
       "      <td>The sandstorms in China are causing a lot of h...</td>\n",
       "      <td>The sandstorms in China are causing a lot of h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#Person1# is surprised at the Olympic Stadium'...</td>\n",
       "      <td>People with respiratory tract infections are a...</td>\n",
       "      <td>The sandstorms in China are causing a lot of h...</td>\n",
       "      <td>The sandstorms in China are causing a lot of h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>#Person2# shows #Person1# around the construct...</td>\n",
       "      <td>The gift was a remote car model and a brand ne...</td>\n",
       "      <td>The gift is a remote car model and a brand new...</td>\n",
       "      <td>The gift is a remote car model and a brand new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>#Person2# introduces the Olympic Stadium's fin...</td>\n",
       "      <td>Francis is happy to see you at his birthday pa...</td>\n",
       "      <td>The gift is a remote car model and a brand new...</td>\n",
       "      <td>The gift is a remote car model and a brand new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>#Person1# wants to create a company and is goi...</td>\n",
       "      <td>The car model is a remote model of the car mod...</td>\n",
       "      <td>The gift is a remote car model and a brand new...</td>\n",
       "      <td>The gift is a remote car model and a brand new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>#Person1# abandons the idea of creating a comp...</td>\n",
       "      <td>The teacher is a little angry about the cheating.</td>\n",
       "      <td>#Person1#: Hi, Steven.</td>\n",
       "      <td>#Person1#: Hi, Steven.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>#Person1# wants to start #Person1#'s own busin...</td>\n",
       "      <td>#Person1#: I made a big mistake. #Person1#: I ...</td>\n",
       "      <td>#Person1#: Hi, Steven.</td>\n",
       "      <td>#Person1#: Hi, Steven.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>#Person2# feels itchy. #Person1# doubts it is ...</td>\n",
       "      <td>#Person1#: Hi, Tony.</td>\n",
       "      <td>#Person1#: Hi, Steven.</td>\n",
       "      <td>#Person1#: Hi, Steven.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#Person1# suspects that #Person2# has chicken ...</td>\n",
       "      <td>#Person1#: I'm sorry, Tom.</td>\n",
       "      <td>The train is about to leave.</td>\n",
       "      <td>The train is about to leave.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             human_baseline_summaries  \\\n",
       "0   Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1   In order to prevent employees from wasting tim...   \n",
       "2   Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3   #Person2# arrives late because of traffic jam....   \n",
       "4   #Person2# decides to follow #Person1#'s sugges...   \n",
       "5   #Person2# complains to #Person1# about the tra...   \n",
       "6   #Person1# tells Kate that Masha and Hero get d...   \n",
       "7   #Person1# tells Kate that Masha and Hero are g...   \n",
       "8   #Person1# and Kate talk about the divorce betw...   \n",
       "9   #Person1# and Brian are at the birthday party ...   \n",
       "10  #Person1# attends Brian's birthday party. Bria...   \n",
       "11  #Person1# has a dance with Brian at Brian's bi...   \n",
       "12  #Person1# is surprised at the Olympic Stadium'...   \n",
       "13  #Person2# shows #Person1# around the construct...   \n",
       "14  #Person2# introduces the Olympic Stadium's fin...   \n",
       "15  #Person1# wants to create a company and is goi...   \n",
       "16  #Person1# abandons the idea of creating a comp...   \n",
       "17  #Person1# wants to start #Person1#'s own busin...   \n",
       "18  #Person2# feels itchy. #Person1# doubts it is ...   \n",
       "19  #Person1# suspects that #Person2# has chicken ...   \n",
       "\n",
       "                             original_model_summaries  \\\n",
       "0             Person1 is having a rash or an allergy.   \n",
       "1   The bill for the laundry service on Nov. 20th ...   \n",
       "2   The bill has been charged for your laundry ser...   \n",
       "3   #Person1#: Good morning. I'm in Room 309. #Per...   \n",
       "4   Steven is upset at his wife for cheating on hi...   \n",
       "5   #Person1: I need your help. #Person2: I need y...   \n",
       "6   #Person1#: My wife has an affair with her secr...   \n",
       "7                       Then, think of famous people.   \n",
       "8   #Person1: Abraham Lincoln. #Person2: I think o...   \n",
       "9   #Person1#: I admire the way he has a great vis...   \n",
       "10  #Person1: I'm going to Hebei for my trip. #Per...   \n",
       "11  The North of China is experiencing severe sand...   \n",
       "12  People with respiratory tract infections are a...   \n",
       "13  The gift was a remote car model and a brand ne...   \n",
       "14  Francis is happy to see you at his birthday pa...   \n",
       "15  The car model is a remote model of the car mod...   \n",
       "16  The teacher is a little angry about the cheating.   \n",
       "17  #Person1#: I made a big mistake. #Person1#: I ...   \n",
       "18                               #Person1#: Hi, Tony.   \n",
       "19                         #Person1#: I'm sorry, Tom.   \n",
       "\n",
       "                             instruct_model_summaries  \\\n",
       "0   #Person1#: I'm scratching so much. I can't sta...   \n",
       "1   #Person1#: Good morning. I'm checking out toda...   \n",
       "2   #Person1#: Good morning. I'm checking out toda...   \n",
       "3   #Person1#: Good morning. I'm checking out toda...   \n",
       "4                        #Person1#: I need your help.   \n",
       "5                        #Person1#: I need your help.   \n",
       "6                        #Person1#: I need your help.   \n",
       "7   I think Abraham Lincoln is a great man or woma...   \n",
       "8   I think Abraham Lincoln is a great man or woma...   \n",
       "9   I think Abraham Lincoln is a great man or woma...   \n",
       "10  The sandstorms in China are causing a lot of h...   \n",
       "11  The sandstorms in China are causing a lot of h...   \n",
       "12  The sandstorms in China are causing a lot of h...   \n",
       "13  The gift is a remote car model and a brand new...   \n",
       "14  The gift is a remote car model and a brand new...   \n",
       "15  The gift is a remote car model and a brand new...   \n",
       "16                             #Person1#: Hi, Steven.   \n",
       "17                             #Person1#: Hi, Steven.   \n",
       "18                             #Person1#: Hi, Steven.   \n",
       "19                       The train is about to leave.   \n",
       "\n",
       "                                 peft_model_summaries  \n",
       "0   #Person1#: I'm scratching so much. I can't sta...  \n",
       "1   #Person1#: Good morning. I'm checking out toda...  \n",
       "2   #Person1#: Good morning. I'm checking out toda...  \n",
       "3   #Person1#: Good morning. I'm checking out toda...  \n",
       "4                        #Person1#: I need your help.  \n",
       "5                        #Person1#: I need your help.  \n",
       "6                        #Person1#: I need your help.  \n",
       "7   I think Abraham Lincoln is a great man or woma...  \n",
       "8   I think Abraham Lincoln is a great man or woma...  \n",
       "9   I think Abraham Lincoln is a great man or woma...  \n",
       "10  The sandstorms in China are causing a lot of h...  \n",
       "11  The sandstorms in China are causing a lot of h...  \n",
       "12  The sandstorms in China are causing a lot of h...  \n",
       "13  The gift is a remote car model and a brand new...  \n",
       "14  The gift is a remote car model and a brand new...  \n",
       "15  The gift is a remote car model and a brand new...  \n",
       "16                             #Person1#: Hi, Steven.  \n",
       "17                             #Person1#: Hi, Steven.  \n",
       "18                             #Person1#: Hi, Steven.  \n",
       "19                       The train is about to leave.  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = dataset['test'][20:40]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:20]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    \n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3346ee9-beec-4905-a37f-6f1e377eb3a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.10551097477743776, 'rouge2': 0.0, 'rougeL': 0.09263905410521184, 'rougeLsum': 0.0929764384861583}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.086949844865672, 'rouge2': 0.0, 'rougeL': 0.06901835846895939, 'rougeLsum': 0.0695237435252868}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.086949844865672, 'rouge2': 0.0, 'rougeL': 0.06901835846895939, 'rougeLsum': 0.0695237435252868}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22360f9f-48d5-478c-91ff-6844c4e36067",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "20\n",
      "['Person1 is having a rash or an allergy.', 'The bill for the laundry service on Nov. 20th is $ 30.']\n"
     ]
    }
   ],
   "source": [
    "print(type(original_model_summaries))\n",
    "print(len(original_model_summaries))\n",
    "print(original_model_summaries[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd483f03-8656-40c5-89ca-000d81a25f66",
   "metadata": {},
   "source": [
    "##### Notice, that PEFT model results are not too bad, while the training process was much easier!\n",
    "\n",
    "##### You already computed ROUGE score on the full dataset, after loading the results from the data/dialogue-summary-training-results.csv file. Load the values for the PEFT model now and check its performance compared to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ac4d980-4806-4207-b55e-9877975e344a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.40810631575616746, 'rouge2': 0.1633255794568712, 'rougeL': 0.32507074586565354, 'rougeLsum': 0.3248950182867091}\n"
     ]
    }
   ],
   "source": [
    "#results:\"Generative-AI-with-LLMs/data/dialogue-summary-training-results.csv\")\n",
    "human_baseline_summaries = results['human_baseline_summaries'].values\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "instruct_model_summaries = results['instruct_model_summaries'].values\n",
    "peft_model_summaries     = results['peft_model_summaries'].values\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a755be4-f44f-4cd5-a64b-daa191cac875",
   "metadata": {},
   "source": [
    "##### The results show less of an improvement over full fine-tuning, but the benefits of PEFT typically outweigh the slightly-lower performance metrics.\n",
    "\n",
    "##### Calculate the improvement of PEFT over the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc9be410-c54d-4bfa-b392-ee0477915bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\n",
      "rouge1: 17.47%\n",
      "rouge2: 8.73%\n",
      "rougeL: 12.36%\n",
      "rougeLsum: 12.34%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40987ecf-d434-40ca-876b-ff0ed3866d24",
   "metadata": {},
   "source": [
    "##### Now calculate the improvement of PEFT over a full fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad88ce0c-9f81-48c6-bc9d-5e4b9ee281be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\n",
      "rouge1: -1.35%\n",
      "rouge2: -1.70%\n",
      "rougeL: -1.34%\n",
      "rougeLsum: -1.35%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0796ebf-b28e-48f1-adc6-200789c8b667",
   "metadata": {},
   "source": [
    "##### Here you see a small percentage decrease in the ROUGE metrics vs. full fine-tuned. However, the training requires much less computing and memory resources (often just a single GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f256bc-44e8-4c44-b590-411e8c43e93a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
