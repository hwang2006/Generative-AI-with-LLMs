{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8d2fd0-ddea-4de3-8547-b93443acd4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade pip\n",
    "#%pip install --disable-pip-version-check \\\n",
    "#    torch==1.13.1 \\\n",
    "#    torchdata==0.5.1 --quiet\n",
    "#\n",
    "#%pip install \\\n",
    "#    transformers==4.27.2 \\\n",
    "#    datasets==2.11.0  --quiet\n",
    "#!conda install --quiet --yes pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia\n",
    "#!pip install --upgrade pip\n",
    "#!pip install torch==2.0.1 torchdata \n",
    "#!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf94136-ca85-49a0-bf41-8f8173ba0516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37745f08-7394-442f-8024-01c464708ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c3231cf-f44c-437d-bbae-cc846ba6d9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31bae0e5-5acd-488a-826c-1212890c7fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12460"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3716a02a-d443-4ac8-8a62-0b8a7a89beab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'test_13_3',\n",
       " 'dialogue': \"#Person1#: What time is it, Tom?\\n#Person2#: Just a minute. It's ten to nine by my watch.\\n#Person1#: Is it? I had no idea it was so late. I must be off now.\\n#Person2#: What's the hurry?\\n#Person1#: I must catch the nine-thirty train.\\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\",\n",
       " 'summary': \"#Person1# is rushing to catch a train but Tom thinks it isn't necessary.\",\n",
       " 'topic': 'discuss leaving'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fdbc937-8a62-4558-b186-110c466a3e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_indices = [40, 200]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d9a69e-7011-4710-aec7-e8801fa987ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 200]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d687fd1-07cd-4080-be0f-eb803ca42a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40, 200]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc4e4721-2a77-4f5d-9a0a-19a9b0d73d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffd6da01-cef8-4aab-a9e5-fd01aa62313e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c69a3330-dc32-4db1-b1b3-3fdd0f4ff1eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:\n",
      "tensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n",
      "\n",
      "DECODED SENTENCE:\n",
      "What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What time is it, Tom?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"][0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('ENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"][0])\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66978019-de55-4897-bd29-042a43e76455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dialogue = dataset['test'][41]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fc3d263-f752-440c-a75b-0419b3ec2a42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#Person1#: What time is it, Tom?\\n#Person2#: Just a minute. It's ten to nine by my watch.\\n#Person1#: Is it? I had no idea it was so late. I must be off now.\\n#Person2#: What's the hurry?\\n#Person1#: I must catch the nine-thirty train.\\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09a0e4d7-49a4-4ef8-a8ac-4f33b878885c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1713,   345, 13515,   536,  4663,    10,   363,    97,    19,    34,\n",
       "             6,  3059,    58,  1713,   345, 13515,   357,  4663,    10,  1142,\n",
       "             3,     9,  1962,     5,    94,    31,     7,     3,   324,    12,\n",
       "          4169,    57,    82,  1605,     5,  1713,   345, 13515,   536,  4663,\n",
       "            10,    27,     7,    34,    58,    27,   141,   150,   800,    34,\n",
       "            47,    78,  1480,     5,    27,   398,    36,   326,   230,     5,\n",
       "          1713,   345, 13515,   357,  4663,    10,   363,    31,     7,     8,\n",
       "         23601,    58,  1713,   345, 13515,   536,  4663,    10,    27,   398,\n",
       "          3579,     8,  4169,    18,    17,  9288,    17,    63,  2412,     5,\n",
       "          1713,   345, 13515,   357,  4663,    10,   148,    31,   162,  2500,\n",
       "            13,    97,   780,     5,    37, 14421,  2478,    19,   182,   885,\n",
       "             5,    94,   751,    31,    17,   240,    72,   145,  6786,   676,\n",
       "            12,   129,   132,     5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29bf04d1-d824-4b1e-b6c1-4ec226238e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6014e5c-57da-43a2-b0b4-2d87c5a8d5dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 5780,  536,   10,   94,   31,    7,    3,  324,   12, 4169,    5,\n",
      "            1]])\n",
      "tensor([   0, 5780,  536,   10,   94,   31,    7,    3,  324,   12, 4169,    5,\n",
      "           1])\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa1e6749-4839-45dd-897c-6d30b6df3516",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person1: It's ten to nine.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31a6b0b6-9563-4ab6-b4d3-cf12b77b95b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6386fa57-e9a2-4322-88f1-f91e259ad716",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "Person1: It's ten to nine.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt').to(\"cuda\")\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79fbf7b5-166a-4240-af57-3f6a0623e1d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d617ed07-5622-4251-a9f7-15d89274904f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSummarize the following conversation.\\n\\n#Person1#: Have you considered upgrading your system?\\n#Person2#: Yes, but I'm not sure what exactly I would need.\\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\\n#Person2#: That would be a definite bonus.\\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\\n#Person2#: How can we do that?\\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\\n#Person2#: No.\\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\\n#Person2#: That sounds great. Thanks.\\n\\nSummary:\\n   \""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f2f0358-1c2b-4f79-990f-1943fa65e84d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "The train is about to leave.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aecabd89-bbfc-4057-8246-8933e084c995",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: It's ten to nine.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7c11099-93bd-4e6c-b796-7c904425cd0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize what is going on based on the following dialogue between two persons.\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Tom is late for the train.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize what is going on based on the following dialogue between two persons.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: Have you considered upgrading your system? #Person2#: Yes, but I'm not sure what exactly I would need. #Person1#: You could add a painting program to your software.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize what is going on based on the following dialogue between two persons.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95487953-c151-42c6-9bc4-40dfcaa13be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Tom is late for the train.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72dc666e-3e5f-4cf1-a993-c2be8b34e06c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Here is a dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What were they talking about?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Tom is late for the train.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Here is a dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What were they talking about?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: You might consider upgrading your system. #Person2#: You might also consider adding a painting program to your software. #Person1#: You might also consider adding a CD-ROM drive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "Here is a dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What were they talking about?\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dbcb596-7ce4-44a9-a706-021fc4939c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92e06f91-6f01-4860-98ea-37e61f2f45b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "\n",
      "What was going on?\n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "\n",
      "What was going on?\n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40, 80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c166e9e5-1662-4d61-b2ab-6dd9dd2022e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt').to(\"cuda\")\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df130f59-d65d-45cd-ad47-26ea3313b4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "#Person1 wants to upgrade his system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#generation_config = GenerationConfig(max_new_tokens=50)\n",
    "generation_config = GenerationConfig(max_new_tokens=10)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt').to(\"cuda\")\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2ae664b-90f7-4885-ad1e-93dd06dee9c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "#Person1 wants to upgrade his system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#generation_config = GenerationConfig(max_new_tokens=50)\n",
    "# generation_config = GenerationConfig(max_new_tokens=10)\n",
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt').to(\"cuda\")\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2739d6c2-cf35-4671-9fd5-abcb280ad0df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "#Person1 recommends adding a painting program to the software.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#generation_config = GenerationConfig(max_new_tokens=50)\n",
    "# generation_config = GenerationConfig(max_new_tokens=10)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt').to(\"cuda\")\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b479dfe4-8a3c-4e03-9a01-57aebbe90266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "#Person1 considers it necessary to upgrade the system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#generation_config = GenerationConfig(max_new_tokens=50)\n",
    "# generation_config = GenerationConfig(max_new_tokens=10)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt').to(\"cuda\")\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0649b527-f42c-4240-ada1-6056a2afbcbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "#Person2 suggests something other than making some designs using your computer hardware.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#generation_config = GenerationConfig(max_new_tokens=50)\n",
    "# generation_config = GenerationConfig(max_new_tokens=10)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.5)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt').to(\"cuda\")\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1956ce84-8d35-44f1-b5a5-fef49d67d858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the following paragraph.\n",
      "\n",
      "Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
      "Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
      "that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
      "started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
      "programmatically through the SageMaker Python SDK.\n",
      "Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
      "ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
      "benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
      "exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
      "state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
      "Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
      "needs for applications such as translation, question answering, and summarizing information.\n",
      "Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
      "US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
      "Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
      "with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
      "please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart \n",
      "Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
      "state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Use the state-of-the-art Falcon LLM model on Amazon SageMaker JumpStart.\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
    "Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
    "that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
    "started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
    "programmatically through the SageMaker Python SDK.\n",
    "Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
    "ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
    "benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
    "exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
    "state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
    "Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
    "needs for applications such as translation, question answering, and summarizing information.\n",
    "Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
    "US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
    "Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
    "with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
    "please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart \n",
    "Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
    "state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker.\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the following paragraph.\n",
    "\n",
    "{paragraph}\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=200\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print(prompt)\n",
    "print(dash_line)\n",
    "print(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7433b08e-0bfb-490e-b8ca-917019b226bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c044b852c443cb98e20dd256df9357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name='tiiuae/falcon-7b-instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a56658d5-d69c-4515-93f7-2d2d7bf8112c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the following paragraph.\n",
      "\n",
      "Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
      "Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
      "that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
      "started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
      "programmatically through the SageMaker Python SDK.\n",
      "Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
      "ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
      "benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
      "exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
      "state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
      "Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
      "needs for applications such as translation, question answering, and summarizing information.\n",
      "Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
      "US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
      "Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
      "with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
      "please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart \n",
      "Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
      "state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker.\n",
      "\n",
      "\n",
      "Summarize the following paragraph.\n",
      "\n",
      "Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
      "Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
      "that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
      "started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
      "programmatically through the SageMaker Python SDK.\n",
      "Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
      "ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
      "benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
      "exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
      "state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
      "Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
      "needs for applications such as translation, question answering, and summarizing information.\n",
      "Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
      "US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
      "Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
      "with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
      "please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart \n",
      "Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
      "state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "Summarize the following paragraph.\n",
      "\n",
      "Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
      "Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
      "that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
      "started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
      "programmatically through the SageMaker Python SDK.\n",
      "Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
      "ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
      "benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
      "exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
      "state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
      "Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
      "needs for applications such as translation, question answering, and summarizing information.\n",
      "Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
      "US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
      "Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
      "with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
      "please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart \n",
      "Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
      "state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker.\n",
      "The text describes the availability of the Falcon 40B language model on Amazon SageMaker JumpStart and its\n",
      "performance in various benchmarks. It also provides a link to a training notebook and blog post on\n",
      "how to use the model.\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
    "Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
    "that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
    "started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
    "programmatically through the SageMaker Python SDK.\n",
    "Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
    "ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
    "benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
    "exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
    "state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
    "Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
    "needs for applications such as translation, question answering, and summarizing information.\n",
    "Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
    "US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
    "Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
    "with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
    "please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart \n",
    "Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
    "state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker.\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the following paragraph.\n",
    "\n",
    "{paragraph}\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "output = tokenizer.decode(\n",
    "    model.to(\"cuda\").generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=50\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "print(dash_line)\n",
    "print(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d894aca-dc86-491d-90ab-fc58673fa9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
